{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33aee812-9149-41e7-a617-50abeac7ef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "Working in folder C:\\Users\\miky_\\Documents\\Python\\PINNS/PINN_HardCons/results/test_eq-example_1_20230801-171042\n",
      "\n",
      "FCN(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=30, bias=True)\n",
      "    (1): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (3): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "| Iter: 0 | Loss: 18.3460 | Error%: 3.122 | Total_time: 0.0min\n",
      "| Iter: 100 | Loss: 0.1071 | Error%: 0.344 | Total_time: 0.1min\n",
      "| Iter: 200 | Loss: 0.1144 | Error%: 0.3308 | Total_time: 0.1min\n",
      "| Iter: 300 | Loss: 0.0063 | Error%: 0.3915 | Total_time: 0.1min\n",
      "| Iter: 400 | Loss: 0.0843 | Error%: 0.3567 | Total_time: 0.2min\n",
      "| Iter: 500 | Loss: 0.0373 | Error%: 0.4458 | Total_time: 0.2min\n",
      "| Iter: 600 | Loss: 0.0020 | Error%: 0.4046 | Total_time: 0.2min\n",
      "| Iter: 700 | Loss: 0.1344 | Error%: 0.3468 | Total_time: 0.3min\n",
      "| Iter: 800 | Loss: 0.0089 | Error%: 0.3743 | Total_time: 0.3min\n",
      "| Iter: 900 | Loss: 0.0123 | Error%: 0.4427 | Total_time: 0.3min\n",
      "| Iter: 1000 | Loss: 0.0137 | Error%: 0.4268 | Total_time: 0.4min\n",
      "| Iter: 1100 | Loss: 0.0205 | Error%: 0.4131 | Total_time: 0.4min\n",
      "| Iter: 1200 | Loss: 0.0361 | Error%: 0.4205 | Total_time: 0.4min\n",
      "| Iter: 1300 | Loss: 0.1106 | Error%: 0.2826 | Total_time: 0.5min\n",
      "| Iter: 1400 | Loss: 0.0062 | Error%: 0.3917 | Total_time: 0.5min\n",
      "| Iter: 1500 | Loss: 0.0034 | Error%: 2.11 | Total_time: 0.5min\n",
      "| Iter: 1600 | Loss: 0.0027 | Error%: 0.4045 | Total_time: 0.6min\n",
      "| Iter: 1700 | Loss: 0.0014 | Error%: 0.4062 | Total_time: 0.6min\n",
      "| Iter: 1800 | Loss: 0.0009 | Error%: 0.4101 | Total_time: 0.6min\n",
      "| Iter: 1900 | Loss: 0.0010 | Error%: 0.407 | Total_time: 0.7min\n",
      "| Iter: 2000 | Loss: 0.0088 | Error%: 0.4067 | Total_time: 0.7min\n",
      "| Iter: 2100 | Loss: 0.0090 | Error%: 0.3859 | Total_time: 0.7min\n",
      "| Iter: 2200 | Loss: 0.0056 | Error%: 0.408 | Total_time: 0.8min\n",
      "| Iter: 2300 | Loss: 0.0112 | Error%: 0.4057 | Total_time: 0.8min\n",
      "| Iter: 2400 | Loss: 0.0046 | Error%: 0.3891 | Total_time: 0.8min\n",
      "| Iter: 2500 | Loss: 0.0026 | Error%: 0.4232 | Total_time: 0.9min\n",
      "| Iter: 2600 | Loss: 0.0015 | Error%: 0.406 | Total_time: 0.9min\n",
      "| Iter: 2700 | Loss: 0.0194 | Error%: 0.3557 | Total_time: 0.9min\n",
      "| Iter: 2800 | Loss: 0.0028 | Error%: 0.3862 | Total_time: 1.0min\n",
      "| Iter: 2900 | Loss: 0.0571 | Error%: 0.3278 | Total_time: 1.0min\n",
      "| Iter: 3000 | Loss: 0.0201 | Error%: 0.6936 | Total_time: 1.0min\n",
      "| Iter: 3100 | Loss: 0.0024 | Error%: 0.4077 | Total_time: 1.1min\n",
      "| Iter: 3200 | Loss: 0.0011 | Error%: 0.4059 | Total_time: 1.1min\n",
      "| Iter: 3300 | Loss: 0.0008 | Error%: 0.4075 | Total_time: 1.2min\n",
      "| Iter: 3400 | Loss: 0.0006 | Error%: 0.4046 | Total_time: 1.2min\n",
      "| Iter: 3500 | Loss: 0.0004 | Error%: 0.4101 | Total_time: 1.2min\n",
      "| Iter: 3600 | Loss: 0.0004 | Error%: 0.409 | Total_time: 1.3min\n",
      "| Iter: 3700 | Loss: 0.0006 | Error%: 0.4169 | Total_time: 1.3min\n",
      "| Iter: 3800 | Loss: 0.0008 | Error%: 0.4238 | Total_time: 1.3min\n",
      "| Iter: 3900 | Loss: 0.0060 | Error%: 0.3938 | Total_time: 1.4min\n",
      "| Iter: 4000 | Loss: 0.0004 | Error%: 0.4012 | Total_time: 1.4min\n",
      "| Iter: 4100 | Loss: 0.0002 | Error%: 0.4108 | Total_time: 1.5min\n",
      "| Iter: 4200 | Loss: 0.0004 | Error%: 0.4033 | Total_time: 1.5min\n",
      "| Iter: 4300 | Loss: 0.0003 | Error%: 0.4121 | Total_time: 1.5min\n",
      "| Iter: 4400 | Loss: 0.0022 | Error%: 0.3937 | Total_time: 1.6min\n",
      "| Iter: 4500 | Loss: 0.0003 | Error%: 0.8771 | Total_time: 1.6min\n",
      "| Iter: 4600 | Loss: 0.0004 | Error%: 0.4081 | Total_time: 1.6min\n",
      "| Iter: 4700 | Loss: 0.0003 | Error%: 0.4086 | Total_time: 1.7min\n",
      "| Iter: 4800 | Loss: 0.0002 | Error%: 0.4092 | Total_time: 1.7min\n",
      "| Iter: 4900 | Loss: 0.0002 | Error%: 0.4092 | Total_time: 1.7min\n",
      "| Iter: 5000 | Loss: 0.0002 | Error%: 0.4097 | Total_time: 1.8min\n",
      "| Iter: 5100 | Loss: 0.0004 | Error%: 0.4062 | Total_time: 1.8min\n",
      "| Iter: 5200 | Loss: 0.0002 | Error%: 0.4134 | Total_time: 1.9min\n",
      "| Iter: 5300 | Loss: 0.0017 | Error%: 0.4125 | Total_time: 1.9min\n",
      "| Iter: 5400 | Loss: 0.0006 | Error%: 0.4156 | Total_time: 1.9min\n",
      "| Iter: 5500 | Loss: 0.0005 | Error%: 0.4052 | Total_time: 2.0min\n",
      "| Iter: 5600 | Loss: 0.0001 | Error%: 0.4119 | Total_time: 2.0min\n",
      "| Iter: 5700 | Loss: 0.0006 | Error%: 0.4066 | Total_time: 2.0min\n",
      "| Iter: 5800 | Loss: 0.0004 | Error%: 0.4149 | Total_time: 2.1min\n",
      "| Iter: 5900 | Loss: 0.0006 | Error%: 0.4066 | Total_time: 2.1min\n",
      "| Iter: 6000 | Loss: 0.0006 | Error%: 0.5431 | Total_time: 2.1min\n",
      "| Iter: 6100 | Loss: 0.0001 | Error%: 0.4097 | Total_time: 2.2min\n",
      "| Iter: 6200 | Loss: 0.0001 | Error%: 0.41 | Total_time: 2.2min\n",
      "| Iter: 6300 | Loss: 0.0001 | Error%: 0.409 | Total_time: 2.2min\n",
      "| Iter: 6400 | Loss: 0.0002 | Error%: 0.4064 | Total_time: 2.3min\n",
      "| Iter: 6500 | Loss: 0.0001 | Error%: 0.4091 | Total_time: 2.3min\n",
      "| Iter: 6600 | Loss: 0.0001 | Error%: 0.4074 | Total_time: 2.3min\n",
      "| Iter: 6700 | Loss: 0.0003 | Error%: 0.4021 | Total_time: 2.4min\n",
      "| Iter: 6800 | Loss: 0.0003 | Error%: 0.4058 | Total_time: 2.4min\n",
      "| Iter: 6900 | Loss: 0.0001 | Error%: 0.4097 | Total_time: 2.5min\n",
      "| Iter: 7000 | Loss: 0.0002 | Error%: 0.405 | Total_time: 2.5min\n",
      "| Iter: 7100 | Loss: 0.0001 | Error%: 0.4097 | Total_time: 2.5min\n",
      "| Iter: 7200 | Loss: 0.0004 | Error%: 0.416 | Total_time: 2.6min\n",
      "| Iter: 7300 | Loss: 0.0002 | Error%: 0.4078 | Total_time: 2.6min\n",
      "| Iter: 7400 | Loss: 0.0002 | Error%: 0.4071 | Total_time: 2.6min\n",
      "| Iter: 7500 | Loss: 0.0002 | Error%: 0.4237 | Total_time: 2.7min\n",
      "| Iter: 7600 | Loss: 0.0001 | Error%: 0.4084 | Total_time: 2.7min\n",
      "| Iter: 7700 | Loss: 0.0001 | Error%: 0.4091 | Total_time: 2.7min\n",
      "| Iter: 7800 | Loss: 0.0001 | Error%: 0.4087 | Total_time: 2.8min\n",
      "| Iter: 7900 | Loss: 0.0001 | Error%: 0.4091 | Total_time: 2.8min\n",
      "| Iter: 8000 | Loss: 0.0001 | Error%: 0.4094 | Total_time: 2.9min\n",
      "| Iter: 8100 | Loss: 0.0001 | Error%: 0.4078 | Total_time: 2.9min\n",
      "| Iter: 8200 | Loss: 0.0001 | Error%: 0.4059 | Total_time: 2.9min\n",
      "| Iter: 8300 | Loss: 0.0001 | Error%: 0.4109 | Total_time: 3.0min\n",
      "| Iter: 8400 | Loss: 0.0001 | Error%: 0.4106 | Total_time: 3.0min\n",
      "| Iter: 8500 | Loss: 0.0001 | Error%: 0.4113 | Total_time: 3.0min\n",
      "| Iter: 8600 | Loss: 0.0001 | Error%: 0.4052 | Total_time: 3.1min\n",
      "| Iter: 8700 | Loss: 0.0001 | Error%: 0.4099 | Total_time: 3.1min\n",
      "| Iter: 8800 | Loss: 0.0001 | Error%: 0.4056 | Total_time: 3.1min\n",
      "| Iter: 8900 | Loss: 0.0001 | Error%: 0.4087 | Total_time: 3.2min\n",
      "| Iter: 9000 | Loss: 0.0001 | Error%: 0.3661 | Total_time: 3.2min\n",
      "| Iter: 9100 | Loss: 0.0001 | Error%: 0.4095 | Total_time: 3.3min\n",
      "| Iter: 9200 | Loss: 0.0000 | Error%: 0.4097 | Total_time: 3.3min\n",
      "| Iter: 9300 | Loss: 0.0001 | Error%: 0.4098 | Total_time: 3.3min\n",
      "| Iter: 9400 | Loss: 0.0001 | Error%: 0.4105 | Total_time: 3.4min\n",
      "| Iter: 9500 | Loss: 0.0000 | Error%: 0.409 | Total_time: 3.4min\n",
      "| Iter: 9600 | Loss: 0.0001 | Error%: 0.41 | Total_time: 3.4min\n",
      "| Iter: 9700 | Loss: 0.0001 | Error%: 0.409 | Total_time: 3.5min\n",
      "| Iter: 9800 | Loss: 0.0001 | Error%: 0.4086 | Total_time: 3.5min\n",
      "| Iter: 9900 | Loss: 0.0001 | Error%: 0.4097 | Total_time: 3.6min\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Libraries\n",
    "#################\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.plots import plot3D_Matrix\n",
    "from utils.utils import create_tests_folder, get_BC_dataset, get_PDE_dataset\n",
    "from utils.gen_plots import generate_gif\n",
    "from utils.gen_data import data_gen\n",
    "from utils.fcn_module import FCN\n",
    "from utils.fcn_module import u_real\n",
    "\n",
    "#from pyDOE import lhs  # Latin Hypercube Sampling\n",
    "\n",
    "\n",
    "#################\n",
    "#  Device configuration\n",
    "#################\n",
    "\n",
    "# Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "# Set the device, if CUDA is installed we use the GPU, otherwise we use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "#################\n",
    "# Tunning Parameters\n",
    "#################\n",
    "\n",
    "steps = 10000  # Number of steps in the optimization\n",
    "batch_size = 1000  # Batch size in the optimization\n",
    "w_bc = 5     # Extra weights for the loss_bc functional\n",
    "w_pde = 1    # Extra weights for the loss_pde functional\n",
    "lr = 1e-1    # Learning rate for the stochastic gradient descent\n",
    "layers = np.array([2, 30, 30, 30, 1]) # Number of neurons on each layer of the fully connected neural network\n",
    "\n",
    "\n",
    "## Choose the example. Each example has different domain and sourse term.\n",
    "case = \"example_1\"  \n",
    "\n",
    "if case == \"example_1\":\n",
    "    domain = [-1, 1, 0, 1]\n",
    "elif case == \"example_2\":\n",
    "    domain = [0, 1, 0, 10]\n",
    "\n",
    "# Define the number of training/test data\n",
    "\n",
    "N_test_x = 150 #  Number of testing points in space\n",
    "N_test_t = 150 #  Number of testing points in time\n",
    "\n",
    "N_train_x = 100 #  Number of training points in space\n",
    "N_train_t = 100 #  Number of training points in time\n",
    "N_bc = 500  #  Number of training points on the boundary\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create a folder with the results    \n",
    "test_folder = create_tests_folder(parent_folder=\"results\", prefix=f\"_eq-{case}\")\n",
    "if not os.path.exists(test_folder):\n",
    "    os.makedirs(test_folder)\n",
    "\n",
    "#################\n",
    "# Generate data\n",
    "#################\n",
    "\n",
    "DG = data_gen(domain, case)  \n",
    "\n",
    "X_train_Nf = DG.get_PDE_dataset(N_train_x, N_train_t)\n",
    "X_train_bc, Y_train_bc = DG.get_BC_dataset(N_bc) # Generate the train dataset (time and space)\n",
    "y_real, x_test, y_test, X, T  = DG.get_test_dataset(N_test_x, N_test_t)\n",
    "\n",
    "##########\n",
    "## Is it possible to use Latin Hypercube Sampling instead of the get_PDE_dataset function\n",
    "#lb = x_test[0]  # first value\n",
    "#ub = x_test[-1]  # last value\n",
    "#X_train_Nf = lb + (ub - lb) * lhs(2, 20000)  # Choose 20000 points, 2 as the inputs are x and t\n",
    "##########\n",
    "\n",
    "\n",
    "# If the device is CUDA (CUDA is installed), the data are sent to the GPU\n",
    "X_train_bc = X_train_bc.float().to(device)  # Training Points (BC)\n",
    "Y_train_bc = Y_train_bc.float().to(device)  # Training Points (BC)\n",
    "X_train_Nf = X_train_Nf.float().to(device)  # Collocation Points\n",
    "X_test = x_test.float().to(device)  # the input dataset (complete)\n",
    "Y_test = y_test.float().to(device)  # the real solution\n",
    "\n",
    "#################\n",
    "# Create Model and Optimazer\n",
    "#################\n",
    "\n",
    "PINN = FCN(layers, case, domain)\n",
    "PINN.to(device)\n",
    "print(PINN)\n",
    "optimizer = torch.optim.Adam(PINN.parameters(), lr=lr, amsgrad=False)\n",
    "\n",
    "# L-BFGS Optimizer\n",
    "# optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr,\n",
    "#                               max_iter = steps,\n",
    "#                               max_eval = None,\n",
    "#                               tolerance_grad = 1e-05,\n",
    "#                               tolerance_change = 1e-09,\n",
    "#                               history_size = 100,\n",
    "#                               line_search_fn = 'strong_wolfe')\n",
    "\n",
    "#################\n",
    "# Training process\n",
    "#################\n",
    "\n",
    "# Creating a root directory to store information\n",
    "res_dict = {\"loss\": [], \"loss_bc\": [], \"loss_pde\": [], \"rela_err_l2\": []} \n",
    "\n",
    "\n",
    "start_time_a = time.time()\n",
    "count = 1\n",
    "for i in range(steps):\n",
    "    if i % 1500 == 0:  # Every 1500 iteration, we halve the rate of learning\n",
    "        lr = lr / 2  \n",
    "        optimizer = torch.optim.Adam(PINN.parameters(), lr=lr, amsgrad=False)\n",
    "    idx = np.random.choice(X_train_Nf.shape[0], batch_size, replace=False) #Choose randomly a batch\n",
    "    loss = PINN.loss(X_train_bc, Y_train_bc, X_train_Nf[idx, :], w_bc=w_bc, w_pde=w_pde)  # use mean squared error\n",
    "    optimizer.zero_grad() # We 'clean' the optimazer\n",
    "    loss.backward()       # Backpropagation (compute the gradients)\n",
    "    optimizer.step()      # Actualization of the parameters\n",
    "    \n",
    "    # optimizer.step(PINN.closure) # L-BFGS Optimizer\n",
    "\n",
    "    # Create the plots with the solution with the current parameters, and the relative error.\n",
    "    res_dict[\"loss\"].append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        rela_err_l2 = PINN.relative_error_l2_norm(X_test, Y_test)\n",
    "        res_dict[\"rela_err_l2\"].append(rela_err_l2.item())\n",
    "    if i % 100 == 0:\n",
    "        u_predict = PINN(X_test)\n",
    "        arr_y1 = u_predict.reshape(shape=[N_test_t, N_test_x]).transpose(1, 0).detach().cpu()\n",
    "        plot3D_Matrix(X, T, arr_y1, name=f\"1_approx_{count}\", rela_e=rela_err_l2, folder=test_folder)\n",
    "        plot3D_Matrix(X, T, arr_y1 - y_real, name=f\"2_error_{count}\", rela_e=rela_err_l2, folder=test_folder)\n",
    "        print(f\"| Iter: {i} | Loss: {loss.detach().cpu().numpy():.4f} | Error%: {rela_err_l2.detach().cpu().numpy():.4g}\" + f\" | Total_time: {(time.time() - start_time_a)/60:.1f}min\")\n",
    "        count += 1\n",
    "    path = test_folder + \"/A_results_dict.npy\"\n",
    "    np.save(path, np.asarray(res_dict, dtype=object))\n",
    "\n",
    "#################\n",
    "# Animation\n",
    "#################\n",
    "    \n",
    "# Generate animations showing the convergence to the real solution, and the relative error  \n",
    "## It is necessary to have the imageio_ffmpeg  (pip install imageio_ffmpeg)\n",
    "generate_gif(test_folder,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d24deb-41c2-45bf-bf76-b676e6704c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
